{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC25 Rush Detection\n",
    "## Model Preparation 2025-02-06\n",
    "This notebook will assist in preparing data for training a model which will detect objects in a screenshot of the game. This notebook assumes an initial model has been trained from ```rush-detection-model.ipynb```.\n",
    "\n",
    "### Flow\n",
    "1. Gather new screenshots using this project\n",
    "2. Run inference from existing model against new screenshots\n",
    "3. Automatically import images and labels into the Label Studio project through API\n",
    "4. Manually verify and append labels\n",
    "5. Repeat\n",
    "\n",
    "### Goal\n",
    "Using this method, I intend to gather:\n",
    "[ ] 1000 examples of ball\n",
    "[ ] 1000 examples of players\n",
    "[ ] 1000 examples of user controlled player\n",
    "\n",
    "## Next Steps\n",
    "After achieving reasonable performance with this model, detection can be extended to pick up a few new objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Environment Setup\n",
    "%pip install huggingface_hub numpy python-dotenv ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather New Screenshots\n",
    "Make sure to run ```screenshot-to-jpg.ipynb``` and ```batch_crop_and_resize_to_height()``` in ```crop-screenshots.ipynb``` to normalize the screenshots to 1080p and crop them to the center of the captured window.  \n",
    "<br>\n",
    "Output directory should be ```'../screenshots/jpg/cropped'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gather new screenshot\n",
    "new_screenshots_location = \"../screenshots/jpg/cropped\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Run inference on new screenshots using existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "###################################################################\n",
    "output_location = os.path.expanduser(\"~/Documents/Rush/inference\")\n",
    "os.makedirs(output_location, exist_ok=True)\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###########################################################\n",
    "model_path = os.path.expanduser(\"~/Documents/Rush/best.pt\")\n",
    "###########################################################\n",
    "\n",
    "# Load the trained YOLO model\n",
    "model = YOLO(model_path)  # Replace with the correct path to your best weights file.\n",
    "\n",
    "# List all image files in the directory\n",
    "image_files = [f for f in os.listdir(new_screenshots_location) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Function to perform inference, display results, and save annotations\n",
    "def run_inference(image_path, save_dir):\n",
    "    # Load image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "    \n",
    "    # Perform inference\n",
    "    results = model(img_rgb, conf=0.2)\n",
    "    \n",
    "    # Render results on the image\n",
    "    annotated_img = results[0].plot()  # Annotate the image with results\n",
    "\n",
    "    # Save annotated image\n",
    "    annotated_image_path = os.path.join(save_dir, os.path.basename(image_path))\n",
    "    plt.imsave(annotated_image_path, annotated_img)  # Save using imsave\n",
    "\n",
    "    # Prepare detection results for Label Studio\n",
    "    detections = []\n",
    "    for result in results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, result.xyxy[0])  # Extract coordinates\n",
    "        confidence = result.conf[0].item()\n",
    "        class_id = int(result.cls[0].item())\n",
    "\n",
    "        # Append detection to list\n",
    "        detections.append({\n",
    "            \"rectanglelabels\": results[0].names[class_id],  # Replace with your class names\n",
    "            \"points\": {\n",
    "                \"x\": x1 / img.shape[1],  # Convert to relative coordinates\n",
    "                \"y\": y1 / img.shape[0],  # Convert to relative coordinates\n",
    "                \"width\": (x2 - x1) / img.shape[1],\n",
    "                \"height\": (y2 - y1) / img.shape[0]\n",
    "            },\n",
    "            \"confidence\": confidence,\n",
    "            \"class_id\": class_id  # Class ID of detected object\n",
    "        })\n",
    "\n",
    "    # Save detection results in a JSON file\n",
    "    json_output_path = os.path.join(save_dir, f\"{os.path.splitext(os.path.basename(image_path))[0]}.json\")\n",
    "    with open(json_output_path, 'w') as json_file:\n",
    "        json.dump(detections, json_file, indent=4)\n",
    "\n",
    "# Run inference for each image in the directory\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(new_screenshots_location, image_file)\n",
    "    run_inference(image_path, output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Studio Import\n",
    "Import the inference results into Label Studio using the API. This assumes you have setup .env in the root of this project with the following keys:\n",
    "```\n",
    "LABEL_STUDIO_URL=http://localhost:8080\n",
    "LABEL_STUDIO_API_TOKEN={api_token}\n",
    "LABEL_STUDIO_PROJECT_ID={project_id}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construct the path to the .env file located two directories up\n",
    "env_path = os.path.join(current_dir, '..', '.env')\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Access values\n",
    "label_studio_url = os.getenv('LABEL_STUDIO_URL')\n",
    "label_studio_api_token = os.getenv('LABEL_STUDIO_API_TOKEN')\n",
    "label_studio_project_id = os.getenv('LABEL_STUDIO_PROJECT_ID')\n",
    "\n",
    "# Create a dictionary of the variables to check\n",
    "env_values = {\n",
    "    \"Label Studio URL\": label_studio_url,\n",
    "    \"Label Studio API Token\": label_studio_api_token,\n",
    "    \"Label Studio Project ID\": label_studio_project_id,\n",
    "}\n",
    "\n",
    "# Check each variable and print its status\n",
    "for name, value in env_values.items():\n",
    "    if value:\n",
    "        print(f\"{name} is set.\")\n",
    "    else:\n",
    "        print(f\"{name} is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# API Headers\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token {label_studio_api_token}\"\n",
    "}\n",
    "\n",
    "def upload_image(image_path):\n",
    "    \"\"\"Uploads an image to Label Studio and returns its task_id.\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        response = requests.post(\n",
    "            f\"{label_studio_url}/api/projects/{label_studio_project_id}/import?return_task_ids=true\",\n",
    "            headers=HEADERS,\n",
    "            files={\"file\": f}\n",
    "        )\n",
    "\n",
    "    if response.status_code == 201:\n",
    "        task_id = response.json().get(\"task_ids\", [None])[0]\n",
    "        if task_id:\n",
    "            return task_id  # Return the task ID\n",
    "    print(f\"❌ Image upload failed for {image_path}: {response.text}\")\n",
    "    return None\n",
    "\n",
    "def upload_tasks(image_dir, label_dir):\n",
    "    \"\"\"Uploads images and their corresponding JSON annotations to Label Studio correctly.\"\"\"\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "    for image_file in tqdm(image_files, desc=\"Uploading images\"):\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        annotation_path = os.path.join(label_dir, os.path.splitext(image_file)[0] + \".json\")\n",
    "\n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"⚠️ No annotation found for {image_file}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Upload image and get its task ID\n",
    "        task_id = upload_image(image_path)\n",
    "        if not task_id:\n",
    "            continue  # Skip if upload failed\n",
    "\n",
    "        # Load annotations\n",
    "        with open(annotation_path, \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "\n",
    "        # ✅ Fix: Convert coordinates to percentages\n",
    "        formatted_annotations = []\n",
    "        for ann in annotations:\n",
    "            formatted_annotations.append({\n",
    "                \"original_width\": 1920,  # Set based on actual image dimensions\n",
    "                \"original_height\": 1080,\n",
    "                \"image_rotation\": 0,\n",
    "                \"from_name\": \"label\", \n",
    "                \"to_name\": \"image\",\n",
    "                \"type\": \"rectanglelabels\",\n",
    "                \"value\": {\n",
    "                    \"x\": ann[\"points\"][\"x\"] * 100,  # Convert to percentage\n",
    "                    \"y\": ann[\"points\"][\"y\"] * 100,\n",
    "                    \"width\": ann[\"points\"][\"width\"] * 100,\n",
    "                    \"height\": ann[\"points\"][\"height\"] * 100,\n",
    "                    \"rotation\": 0,\n",
    "                    \"rectanglelabels\": [ann[\"rectanglelabels\"]] \n",
    "                }\n",
    "            })\n",
    "\n",
    "        annotation_payload = {\n",
    "            \"result\": formatted_annotations,\n",
    "            \"was_cancelled\": False,\n",
    "            \"ground_truth\": False\n",
    "        }\n",
    "\n",
    "        # Create a new annotation for the task\n",
    "        response = requests.post(\n",
    "            f\"{label_studio_url}/api/tasks/{task_id}/annotations/\",\n",
    "            headers={**HEADERS, \"Content-Type\": \"application/json\"},\n",
    "            json=annotation_payload\n",
    "        )\n",
    "\n",
    "        if response.status_code >= 300:\n",
    "            print(f\"❌ Error creating annotation for task {task_id}: {response.text}\")\n",
    "\n",
    "# Run the function\n",
    "upload_tasks(new_screenshots_location, output_location)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
